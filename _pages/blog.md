---
permalink: /blog/
title: "Latest"
author_profile: true
---

## Kaggle Slayer: Introduction to XGBoost

<img src="/images/dragon_slayer.png" alt="Dragon Slayer" class="float-right"/>

Every Kaggle competition presents unique obstacles - requiring the sharpest of skills and the most powerful of tools. In this post, I will introduce you to one of the greatest weapons, which every data scientist should have in their arsenal: <span style="color:#007BFF;">XGBoost</span>. 

Short for “Extreme Gradient Boosting”, XGBoost is a highly scalable, efficient decision tree machine learning library that makes use of decision trees, the gradient boosting framework, and ensemble learning to provide excellent results in classification and regression tasks. If you're just entering the battlefield of machine learning, this tutorial will teach you the basics of XGBoost and help you begin using it to conquer your next competition.

### Foundations

The term “gradient boosting” comes from the concept of “boosting” — improving a single weak model (in this case, a decision tree) by combining it with other weak models to form a collectively strong model.

In gradient boosting, an ensemble of shallow decision trees is iteratively trained, with each tree focusing on correcting the residual errors of the previous model. The final prediction is a weighted sum of all the tree predictions, creating a robust model from individually weak learners.





