---
permalink: /blog/
title: "Latest"
author_profile: true
---

## Kaggle Slayer: Introduction to XGBoost

<img src="/images/dragon_slayer.png" alt="Dragon Slayer" class="float-right"/>

Every Kaggle competition presents unique obstacles - requiring the sharpest of skills and the most powerful of tools. In this post, I will introduce you to one of the greatest weapons, which every data scientist should have in their arsenal: <span style="color:#007BFF;">XGBoost</span>. 

Short for “Extreme Gradient Boosting”, XGBoost is a highly scalable, efficient decision tree machine learning library that makes use of the gradient boosting framework to provide excellent results in classification and regression tasks. If you're just entering the battlefield of machine learning, this tutorial will teach you the basics of XGBoost and help you begin using it to conquer your next challenge.

### Foundations

The term “gradient boosting” comes from the concept of “boosting” — improving a single weak model (in this case, a decision tree) by combining it with other weak models to form a collectively strong model.

In gradient boosting, an ensemble of shallow decision trees is iteratively trained, with each tree focusing on correcting the residual errors of the previous model. The final prediction is a weighted sum of all the tree predictions, creating a robust model from individually weak learners.

If you are wanting to learn more about any of these concepts, I highly recommend checking out the Youtube channel "StatQuest with Josh Starmer". He offers a ton of simple yet thorough videos on machine learning. Here are a few of his playlists that explain the foundational components of XGBoost:

* [Decision Trees](https://www.youtube.com/watch?v=_L39rN6gz7Y&list=PLblh5JKOoLUKAtDViTvRGFpphEc24M-QH)
* [Gradient Boosting](https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6)

### Why XGBoost?






